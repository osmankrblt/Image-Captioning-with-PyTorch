{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hosma\\anaconda3\\envs\\torchEnv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from  matplotlib import pyplot as plt\n",
    "import shutil\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import json \n",
    "from time import time\n",
    "import pickle \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from utils import save_checkpoint, load_checkpoint, print_examples\n",
    "from imageCaptionModel import CNNtoRNN\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import optim \n",
    "from torchvision.models import EfficientNet_B1_Weights\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "import wandb\n",
    "import os\n",
    "from cv2 import threshold\n",
    "import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): \n",
    "    device = \"cuda:0\" \n",
    "else: \n",
    "    device = \"cpu\" \n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "load_model = False\n",
    "save_model = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficentB1Transform = EfficientNet_B1_Weights.IMAGENET1K_V2.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_loader import get_loader\n",
    "\n",
    "\n",
    "train_loader, dataset = get_loader(\n",
    "        root_folder=\"Flickr8K/Images\",\n",
    "        annotation_file=\"Flickr8K/captions.txt\",\n",
    "        transform=efficentB1Transform,\n",
    "        num_workers=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, captions = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1],\n",
       "        [   4,  149,    4,  178,    4,    4,    4,   10,    4,    6,    4,    4,\n",
       "           50,   71,    4,   50,    4,    4,    4,    4,    4,    4,    4,    4,\n",
       "            4,    4,    4,    4,    4,   10,   14,    4],\n",
       "        [   9, 1874,   14,  453,    9,   20,   14,   80,   56,   69,   14,   82,\n",
       "           97,   61,   61,   68,    6,   14,   30,    6,  525,   30,   30,    9,\n",
       "          191,  638,  183,   80,  464,   50,   17,    6],\n",
       "        [  43,   13,    8,  108,    7,    6,   12,   17,    8,    8,   17,  291,\n",
       "           34,   97,   43,  153,  624,   17,   16,  108, 1112,    6,    6,    7,\n",
       "           12, 1122,   79, 1911,  384,   97,   46,   75],\n",
       "        [ 173,  711,  272, 1627,  582,  442,    4,   79,    4,   31,    3,   13,\n",
       "           69,    8,   13,   35,    4,  264,   21,   40,   13,   16,   92,   75,\n",
       "          136,  255,    4,    4,    4,   34,    8,    8],\n",
       "        [ 188,    5,    3,    5,   13,    4,  141,    4,  619, 1469,   10,  150,\n",
       "           13,  310,    4,    4,  195,    8,  720,    4,    4,    4,    8,    8,\n",
       "         1510,    4,   18,  288,  570,    8,    4,    4],\n",
       "        [ 375,    2,   73,    2,    4,   59,  508,   20,  364,    5,  926, 1489,\n",
       "            4,  485,  815,   26,   36,    4,  235,  261,  222, 2255,   10,    4,\n",
       "         1601,  207,   16,    8,    5,   10,   57,   42],\n",
       "        [ 376,    0,   84,    0,  475,   13,  463,   18,   17,    2,   11,  449,\n",
       "           58,   34,  726, 2895,    5,  399,   48,    8,    5,  823,  192,  827,\n",
       "            2,   36,  431,   10,    2,  213,  287,   12],\n",
       "        [   8,    0,   88,    0,   55,   10,   69,   12,  384,    0,    4,    5,\n",
       "          530,   46, 1197,    5,    2,  244,  236,   10,    2,    4,   35,   70,\n",
       "            0,    8,  202,  121,    0,   12,   70,    4],\n",
       "        [  10,    0,    5,    0,   47,   22,    8,   24,    4,    0,  222,    2,\n",
       "          172,   13,   10,    2,    0,   73,    5,  284,    0,    3,    4,    5,\n",
       "            0,   10,   75,    5,    0,    3,   35,  207],\n",
       "        [ 426,    0,    2,    0,   27,    5,   23,  365,  222,    0,  433,    0,\n",
       "            5,    4,   36,    0,    0,   68,    2,    5,    0,  419,   14,    2,\n",
       "            0,   31,   13,    2,    0, 2599,   53,   36],\n",
       "        [   5,    0,    0,    0,    4,    2,   11,  199,  537,    0,    4,    0,\n",
       "            2,   15,    5,    0,    0, 1232,    0,    2,    0,    4,    8,    0,\n",
       "            0,    5,    4,    0,    0,    5, 2576,    8],\n",
       "        [   2,    0,    0,    0,   57,    0,    4,   13,  109,    0, 2366,    0,\n",
       "            0, 1135,    2,    0,    0,  102,    0,    0,    0,  473,    4,    0,\n",
       "            0,    2,   33,    0,    0,    2,    5,   60],\n",
       "        [   0,    0,    0,    0,  467,    0,    3,   64,   11,    0,  403,    0,\n",
       "            0,  285,    0,    0,    0,   86,    0,    0,    0,    5,   58,    0,\n",
       "            0,    0,   19,    0,    0,    0,    2,   62],\n",
       "        [   0,    0,    0,    0,  935,    0,    5,    5,    4,    0,    5,    0,\n",
       "            0,    5,    0,    0,    0,    5,    0,    0,    0,    2,  369,    0,\n",
       "            0,    0,   10,    0,    0,    0,    0,    5],\n",
       "        [   0,    0,    0,    0,    4,    0,    2,    2,   55,    0,    2,    0,\n",
       "            0,    2,    0,    0,    0,    2,    0,    0,    0,    0,    5,    0,\n",
       "            0,    0,   52,    0,    0,    0,    0,    2],\n",
       "        [   0,    0,    0,    0,  225,    0,    0,    0,    5,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    2,    0,\n",
       "            0,    0,    5,    0,    0,    0,    0,    0],\n",
       "        [   0,    0,    0,    0,   28,    0,    0,    0,    2,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    2,    0,    0,    0,    0,    0],\n",
       "        [   0,    0,    0,    0,   77,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   0,    0,    0,    0,    5,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   0,    0,    0,    0,    2,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hosma\\anaconda3\\envs\\torchEnv\\lib\\site-packages\\torchvision\\models\\_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hosma\\anaconda3\\envs\\torchEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1150: UserWarning: expandable_segments not supported on this platform (Triggered internally at ..\\c10/cuda/CUDAAllocatorConfig.h:30.)\n",
      "  return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n"
     ]
    }
   ],
   "source": [
    "embed_size=256\n",
    "hidden_size=256\n",
    "num_layers = 1\n",
    "vocab_size = len(dataset.vocab)\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 100\n",
    "model = CNNtoRNN(embed_size,hidden_size,vocab_size,num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mh-osmankarabulut\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "\n",
    "def train(model, train_dataloader, epochs=100, learning_rate=5e-5, save_path='./Models/'):\n",
    "    # WandB Başlat\n",
    "    wandb.init(project=\"Image Caption\",settings=wandb.Settings(start_method=\"thread\"))\n",
    "    wandb.watch(model, log=\"all\")\n",
    "\n",
    "    step = 0\n",
    "\n",
    "    if load_model:\n",
    "        step = load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    # Modeli Eğitim Moduna Al\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        if save_model:\n",
    "            checkpoint = {\n",
    "                \"state_dict\":model.state_dict(),\n",
    "                \"optimizer\":optimizer.state_dict(),\n",
    "                \"step\":step\n",
    "            }\n",
    "\n",
    "            save_checkpoint(checkpoint)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        epoch_loss = 0\n",
    "        print(\"Training\")\n",
    "        model.train()\n",
    "        for images, captions in tqdm.tqdm(train_dataloader):\n",
    "            \n",
    "            \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Veriyi ve hedefi al\n",
    "            images = images.to(device)\n",
    "            captions = torch.tensor(captions).to(device)\n",
    "\n",
    "            outputs = model(images, captions[:-1])\n",
    "            loss = criterion(\n",
    "                outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1)\n",
    "            )\n",
    "\n",
    "            \n",
    "            step += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Ortalama Eğitim Kaybını Loglama\n",
    "        avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "        wandb.log({\"epoch_loss\": avg_epoch_loss})\n",
    "\n",
    "      \n",
    "\n",
    "        # Modeli Kaydetme\n",
    "        checkpoint_path = os.path.join(save_path, f\"epoch_{epoch + 1}.pt\")\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        wandb.save(checkpoint_path)\n",
    "\n",
    "    print(\"Eğitim Tamamlandı\")\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "\n",
    "# Eğitim Fonksiyonunu Çağırma\n",
    "#train(model, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"Models\\epoch_100.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1 CORRECT: Dog on a beach by the ocean\n",
      "Example 1 OUTPUT: <SOS> a dog is running through the water . <EOS>\n",
      "Example 2 CORRECT: Child holding red frisbee outdoors\n",
      "Example 2 OUTPUT: <SOS> a little girl in a pink dress is playing with a red ball . <EOS>\n",
      "Example 3 CORRECT: Bus driving by parked cars\n",
      "Example 3 OUTPUT: <SOS> a man in a red shirt is standing in front of a skyscraper . <EOS>\n",
      "Example 4 CORRECT: A small boat in the ocean\n",
      "Example 4 OUTPUT: <SOS> a dog is running through the snow . <EOS>\n",
      "Example 5 CORRECT: A cowboy riding a horse in the desert\n",
      "Example 5 OUTPUT: <SOS> a man in a black shirt is standing in front of a white building . <EOS>\n"
     ]
    }
   ],
   "source": [
    "print_examples(model, device, efficentB1Transform, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
